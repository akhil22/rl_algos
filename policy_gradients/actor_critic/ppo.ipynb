{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1daaaf1f90>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gym\n",
    "import torch\n",
    "import gym\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.distributions import Normal\n",
    "import scipy.signal\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "torch.manual_seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, obs_dimension, sizes):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        sizes = [obs_dimension] + sizes + [1]\n",
    "        act = nn.ReLU\n",
    "        out_activation = nn.Identity\n",
    "        self.layers = []\n",
    "        for j in range(0,len(sizes) - 1):\n",
    "            act_l = act if j < len(sizes) -2 else out_activation\n",
    "            self.layers+=[nn.Linear(sizes[j], sizes[j+1]), act_l()]\n",
    "        self.v = nn.Sequential(*self.layers)\n",
    "    def forward(self, x):\n",
    "        return self.v(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetworkCat(nn.Module):\n",
    "    def __init__(self, obs_dimension, sizes, action_dimension):\n",
    "        super(PolicyNetworkCat, self).__init__()\n",
    "        sizes = [obs_dimension] + sizes + [action_dimension]\n",
    "        act = nn.ReLU\n",
    "        out_activation = nn.Identity\n",
    "        self.layers = []\n",
    "        for j in range(0,len(sizes) - 1):\n",
    "            act_l = act if j < len(sizes) -2 else out_activation\n",
    "            self.layers+=[nn.Linear(sizes[j], sizes[j+1]), act_l()]\n",
    "        self.pi = nn.Sequential(*self.layers)\n",
    "    def forward(self, x):\n",
    "        score = self.pi(x)\n",
    "        probs = F.softmax(score,dim = 1)\n",
    "        dist = torch.distributions.Categorical(probs=probs)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetworkGauss(nn.Module):\n",
    "    def __init__(self, obs_dimension, sizes, action_dimension):\n",
    "        super(PolicyNetworkGauss, self).__init__()\n",
    "        sizes = [obs_dimension] + sizes + [action_dimension]\n",
    "        act = nn.ReLU\n",
    "        out_activation = nn.Identity\n",
    "        self.layers = []\n",
    "        for j in range(0,len(sizes) - 1):\n",
    "            act_l = act if j < len(sizes) -2 else out_activation\n",
    "            self.layers+=[nn.Linear(sizes[j], sizes[j+1]), act_l()]\n",
    "        self.mu = nn.Sequential(*self.layers)\n",
    "        log_std = -0.5*np.ones(action_dimension, dtype=np.float32)\n",
    "        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std))\n",
    "    def forward(self, x):\n",
    "        mean = self.mu(x)\n",
    "        std = torch.exp(self.log_std)\n",
    "        dist = Normal(mean, std)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "sizes = [128]\n",
    "obs_dimension = env.observation_space.shape\n",
    "action_dimension = env.action_space.shape\n",
    "print(obs_dimension)\n",
    "print(env.action_space.shape)\n",
    "v = ValueNetwork(*obs_dimension, sizes)\n",
    "pi = PolicyNetworkGauss(*obs_dimension, sizes, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOBuffer:\n",
    "    \"\"\"\n",
    "    A buffer for storing trajectories experienced by a PPO agent interacting\n",
    "    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
    "    for calculating the advantages of state-action pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros((size, act_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        \"\"\"\n",
    "        Append one timestep of agent-environment interaction to the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Call this at the end of a trajectory, or when one gets cut off\n",
    "        by an epoch ending. This looks back in the buffer to where the\n",
    "        trajectory started, and uses rewards and value estimates from\n",
    "        the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
    "        as well as compute the rewards-to-go for each state, to use as\n",
    "        the targets for the value function.\n",
    "\n",
    "        The \"last_val\" argument should be 0 if the trajectory ended\n",
    "        because the agent reached a terminal state (died), and otherwise\n",
    "        should be V(s_T), the value function estimated for the last state.\n",
    "        This allows us to bootstrap the reward-to-go calculation to account\n",
    "        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
    "        \"\"\"\n",
    "\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "        \n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
    "        \n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n",
    "        \n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        Call this at the end of an epoch to get all of the data from\n",
    "        the buffer, with advantages appropriately normalized (shifted to have\n",
    "        mean zero and std one). Also, resets some pointers in the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # the next two lines implement the advantage normalization trick\n",
    "        self.adv_buf = (self.adv_buf - self.adv_buf.mean()) / self.adv_buf.std()\n",
    "        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf,\n",
    "                    adv=self.adv_buf, logp=self.logp_buf)\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in data.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo(env, seed = 0, buff_size = 100, train_time_steps = 50000, gamma = 0.99, clip_ratio = 0.2, lr_pi = 1e-3, \n",
    "        lr_vf = 1e-2, pi_train_itrs = 10, v_train_itrs = 10, lam = 0.97, max_ep_len = 500):\n",
    "        obs_dim = env.observation_space.shape\n",
    "        action_dim = 2\n",
    "        h_sizes = [64]\n",
    "        vi = ValueNetwork(*obs_dim, h_sizes).to(device)\n",
    "        pi = PolicyNetworkGauss(*obs_dim, h_sizes, action_dim).to(device)\n",
    "        data_buff = PPOBuffer(*obs_dim, action_dim, buff_size)\n",
    "        obs = env.reset()\n",
    "        obs = torch.from_numpy(obs).float().unsqueeze(0).to(device)\n",
    "        curr_time_step = 0\n",
    "        m = pi(obs)\n",
    "        for curr_time_step in tqdm(range(train_time_steps)):\n",
    "                for t in range(0, buff_size):\n",
    "                      #  m = pi(obs)\n",
    "                        m = pi(obs)\n",
    "                        curr_time_step+=1\n",
    "                        b = m.sample()\n",
    "                        a = 0\n",
    "                        #logp = m.log_prob(a)\n",
    "                        obs_new, rew, done, _ = env.step(a)\n",
    "                       # obs = obs_new\n",
    "                      #  v = vi(obs)\n",
    "                        continue\n",
    "                        #data_buff.store(obs, a.detach().numpy(), rew, v.detach().numpy(), logp.detach().numpy())\n",
    "                        curr_time_step+=1\n",
    "                        if done or t == buff_size-1:\n",
    "                                if done:\n",
    "                                        v_ = 0.\n",
    "                                       # print('done')\n",
    "                                        obs = env.reset()\n",
    "                                else:\n",
    "                                        v_ = vi(torch.from_numpy(obs_new).float().unsqueeze(0))\n",
    "                                        v_ = v_.detach().numpy()\n",
    "                                #data_buff.finish_path(v_)\n",
    "                #data = data_buff.get()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]/home/akhil/anaconda3/envs/ppo/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py:150: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n",
      "  1%|          | 304/50000 [00:03<10:25, 79.41it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/akhil/rl_algos/policy_gradients/actor_critic/ppo.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/akhil/rl_algos/policy_gradients/actor_critic/ppo.ipynb#ch0000011?line=0'>1</a>\u001b[0m ppo(env)\n",
      "\u001b[1;32m/home/akhil/rl_algos/policy_gradients/actor_critic/ppo.ipynb Cell 8\u001b[0m in \u001b[0;36mppo\u001b[0;34m(env, seed, buff_size, train_time_steps, gamma, clip_ratio, lr_pi, lr_vf, pi_train_itrs, v_train_itrs, lam, max_ep_len)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akhil/rl_algos/policy_gradients/actor_critic/ppo.ipynb#ch0000011?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m curr_time_step \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(train_time_steps)):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akhil/rl_algos/policy_gradients/actor_critic/ppo.ipynb#ch0000011?line=13'>14</a>\u001b[0m         \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, buff_size):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akhil/rl_algos/policy_gradients/actor_critic/ppo.ipynb#ch0000011?line=14'>15</a>\u001b[0m               \u001b[39m#  m = pi(obs)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/akhil/rl_algos/policy_gradients/actor_critic/ppo.ipynb#ch0000011?line=15'>16</a>\u001b[0m                 m \u001b[39m=\u001b[39m pi(obs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akhil/rl_algos/policy_gradients/actor_critic/ppo.ipynb#ch0000011?line=16'>17</a>\u001b[0m                 curr_time_step\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akhil/rl_algos/policy_gradients/actor_critic/ppo.ipynb#ch0000011?line=17'>18</a>\u001b[0m                 b \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39msample()\n",
      "File \u001b[0;32m~/anaconda3/envs/ppo/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/akhil/rl_algos/policy_gradients/actor_critic/ppo.ipynb Cell 8\u001b[0m in \u001b[0;36mPolicyNetworkGauss.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akhil/rl_algos/policy_gradients/actor_critic/ppo.ipynb#ch0000011?line=14'>15</a>\u001b[0m mean \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmu(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akhil/rl_algos/policy_gradients/actor_critic/ppo.ipynb#ch0000011?line=15'>16</a>\u001b[0m std \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_std)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/akhil/rl_algos/policy_gradients/actor_critic/ppo.ipynb#ch0000011?line=16'>17</a>\u001b[0m dist \u001b[39m=\u001b[39m Normal(mean, std)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/akhil/rl_algos/policy_gradients/actor_critic/ppo.ipynb#ch0000011?line=17'>18</a>\u001b[0m \u001b[39mreturn\u001b[39;00m dist\n",
      "File \u001b[0;32m~/anaconda3/envs/ppo/lib/python3.8/site-packages/torch/distributions/normal.py:45\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, loc, scale, validate_args\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 45\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale \u001b[39m=\u001b[39m broadcast_all(loc, scale)\n\u001b[1;32m     46\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(loc, Number) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(scale, Number):\n\u001b[1;32m     47\u001b[0m         batch_shape \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mSize()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ppo(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ppo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "122de4a4be327aa1ba4f735a2434ab4bffab9a9a54192dfc85777065cfde1c01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
