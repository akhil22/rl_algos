{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import torch\n",
    "import gym\n",
    "import torch\n",
    "import gym\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.distributions import Normal\n",
    "import scipy.signal\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "#torch.manual_seed(100)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_cumsum(x, discount):\n",
    "    \"\"\"\n",
    "    magic from rllab for computing discounted cumulative sums of vectors.\n",
    "    input: \n",
    "        vector x, \n",
    "        [x0, \n",
    "         x1, \n",
    "         x2]\n",
    "    output:\n",
    "        [x0 + discount * x1 + discount^2 * x2,  \n",
    "         x1 + discount * x2,\n",
    "         x2]\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, obs_dimension, sizes, act = nn.ReLU):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        sizes = [obs_dimension] + sizes + [1]\n",
    "        out_activation = nn.Identity\n",
    "        self.layers = []\n",
    "        for j in range(0,len(sizes) - 1):\n",
    "            act_l = act if j < len(sizes) -2 else out_activation\n",
    "            self.layers+=[nn.Linear(sizes[j], sizes[j+1]), act_l()]\n",
    "        self.v = nn.Sequential(*self.layers)\n",
    "    def forward(self, x):\n",
    "        return self.v(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetworkCat(nn.Module):\n",
    "    def __init__(self, obs_dimension, sizes, action_dimension, act= nn.ReLU):\n",
    "        super(PolicyNetworkCat, self).__init__()\n",
    "        sizes = [obs_dimension] + sizes + [action_dimension]\n",
    "        out_activation = nn.Identity\n",
    "        self.layers = []\n",
    "        for j in range(0,len(sizes) - 1):\n",
    "            act_l = act if j < len(sizes) -2 else out_activation\n",
    "            self.layers+=[nn.Linear(sizes[j], sizes[j+1]), act_l()]\n",
    "        self.pi = nn.Sequential(*self.layers)\n",
    "    def forward(self, x):\n",
    "        score = self.pi(x)\n",
    "        probs = F.softmax(score,dim = 1)\n",
    "        dist = torch.distributions.Categorical(probs=probs)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetworkGauss(nn.Module):\n",
    "    def __init__(self, obs_dimension, sizes, action_dimension):\n",
    "        super(PolicyNetworkGauss, self).__init__()\n",
    "        sizes = [obs_dimension] + sizes + [action_dimension]\n",
    "        act = nn.Tanh\n",
    "        out_activation = nn.Identity\n",
    "        self.layers = []\n",
    "        for j in range(0,len(sizes) - 1):\n",
    "            act_l = act if j < len(sizes) -2 else out_activation\n",
    "            self.layers+=[nn.Linear(sizes[j], sizes[j+1]), act_l()]\n",
    "        self.mu = nn.Sequential(*self.layers)\n",
    "        log_std = -0.5*np.ones(action_dimension, dtype=np.float32)\n",
    "        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std))\n",
    "    def forward(self, x):\n",
    "        mean = self.mu(x)\n",
    "        std = torch.exp(self.log_std)\n",
    "        dist = Normal(mean, std)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(4,)\n2\n"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "sizes = [128]\n",
    "obs_dimension = env.observation_space.shape\n",
    "action_dimension = env.action_space.shape\n",
    "print(obs_dimension)\n",
    "print(env.action_space.n)\n",
    "v = ValueNetwork(*obs_dimension, sizes)\n",
    "pi = PolicyNetworkGauss(*obs_dimension, sizes, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOBuffer:\n",
    "    \"\"\"\n",
    "    A buffer for storing trajectories experienced by a PPO agent interacting\n",
    "    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
    "    for calculating the advantages of state-action pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros((size,), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        \"\"\"\n",
    "        Append one timestep of agent-environment interaction to the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Call this at the end of a trajectory, or when one gets cut off\n",
    "        by an epoch ending. This looks back in the buffer to where the\n",
    "        trajectory started, and uses rewards and value estimates from\n",
    "        the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
    "        as well as compute the rewards-to-go for each state, to use as\n",
    "        the targets for the value function.\n",
    "\n",
    "        The \"last_val\" argument should be 0 if the trajectory ended\n",
    "        because the agent reached a terminal state (died), and otherwise\n",
    "        should be V(s_T), the value function estimated for the last state.\n",
    "        This allows us to bootstrap the reward-to-go calculation to account\n",
    "        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
    "        \"\"\"\n",
    "\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "       # print(rews)\n",
    "       # print(vals)\n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
    "        \n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n",
    "        \n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        Call this at the end of an epoch to get all of the data from\n",
    "        the buffer, with advantages appropriately normalized (shifted to have\n",
    "        mean zero and std one). Also, resets some pointers in the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # the next two lines implement the advantage normalization trick\n",
    "\n",
    "        self.adv_buf = (self.adv_buf - self.adv_buf.mean()) / self.adv_buf.std()\n",
    "        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf,\n",
    "                    adv=self.adv_buf, logp=self.logp_buf)\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in data.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo(env, seed = 0, buff_size = 4000, train_time_steps = 1000000, gamma = 0.99, clip_ratio = 0.2, lr_pi = 3e-4, \n",
    "        lr_vf = 1e-3, pi_train_itrs = 80, v_train_itrs = 80, lam = 0.97, max_ep_len = 500):\n",
    "        obs_dim = env.observation_space.shape\n",
    "        action_dim = 2\n",
    "        h_sizes = [64]\n",
    "        vi = ValueNetwork(*obs_dim, h_sizes).to(device)\n",
    "        pi = PolicyNetworkCat(*obs_dim, h_sizes, action_dim).to(device)\n",
    "        data_buff = PPOBuffer(*obs_dim, 1, buff_size)\n",
    "        policy_opt = optim.Adam(pi.parameters(), lr = lr_pi)\n",
    "        value_opt = optim.Adam(vi.parameters(), lr = lr_vf)\n",
    "        obs = env.reset()\n",
    "        curr_time_step = 0\n",
    "        #pbar = tqdm(total = train_time_steps)\n",
    "        num_episode = 0\n",
    "        ep_rewards = [0]\n",
    "        while curr_time_step < train_time_steps: \n",
    "                for t in range(0, buff_size):\n",
    "                        obs_tensor = torch.from_numpy(obs).float().unsqueeze(0).to(device)\n",
    "                        with torch.no_grad():\n",
    "                                m = pi(obs_tensor)\n",
    "                                a = m.sample()\n",
    "                       # print(a.detach().numpy())\n",
    "                       # print(m)\n",
    "                        #return\n",
    "                                logp = m.log_prob(a)\n",
    "                                obs_new, rew, done, _ = env.step(a.item())\n",
    "                                ep_rewards[num_episode]+=rew \n",
    "                                v = vi(obs_tensor)\n",
    "                                obs = obs_new\n",
    "                        data_buff.store(obs, a.numpy(), rew, v.numpy(), logp.numpy())\n",
    "                        if done or t == buff_size-1:\n",
    "                                if done:\n",
    "                                        v_ = 0.\n",
    "                                       # print('done')\n",
    "                                        obs = env.reset()\n",
    "                                        done = False\n",
    "                                        num_episode+=1\n",
    "                                        ep_rewards.append(0)\n",
    "                                        if num_episode %100 == 0:\n",
    "                                                print(f'episode: {num_episode-1} \\t episode_reward: {np.mean(ep_rewards[-10:-2])} \\t total steps:{curr_time_step}')\n",
    "                                else:\n",
    "                                        v_ = vi(torch.from_numpy(obs).float().unsqueeze(0))\n",
    "                                        v_ = v_.detach().numpy()\n",
    "                                data_buff.finish_path(v_)\n",
    "                        curr_time_step+=1\n",
    "                       # pbar.update(1)\n",
    "                data = data_buff.get()\n",
    "                for j in range(0, pi_train_itrs):\n",
    "                        policy_opt.zero_grad()\n",
    "                        act, adv, o, logp_old= data['act'], data['adv'], data['obs'], data['logp']\n",
    "                        act_dist = pi(o)\n",
    "                        logp = act_dist.log_prob(act)\n",
    "                        ratio = torch.exp(logp - logp_old)\n",
    "                       # print(ratio)\n",
    "                        clip_adv = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio) * adv\n",
    "                        loss_pi = -(torch.min(ratio * adv, clip_adv)).mean()\n",
    "                        print(f'loss pi: {loss_pi}')\n",
    "                        print(f'adv: {adv}')\n",
    "                       # print(\"action\")\n",
    "                       # print(act)\n",
    "                       # print(\"dist\")\n",
    "                       # print(act_dist)\n",
    "                        #policy_loss = -(logp*adv).mean()\n",
    "                        #policy_loss.backward()\n",
    "                        loss_pi.backward()\n",
    "                        policy_opt.step()\n",
    "                for i in range (0, v_train_itrs):\n",
    "                        value_opt.zero_grad()\n",
    "                        ret, ob = data['ret'], data['obs']\n",
    "                        val = vi(ob)\n",
    "                        value_loss = F.mse_loss(val, ret)\n",
    "                       # print(f'value_loss: {value_loss}')\n",
    "                        value_loss.backward()\n",
    "                        value_opt.step()\n",
    "               # print(curr_time_step)\n",
    "                #pbar.update(1)\n",
    "       # pbar.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.11435288935899734\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.114400215446949\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.11443979293107986\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.11448216438293457\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.11453283578157425\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.11459033936262131\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.11465239524841309\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.11471326649188995\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.1147678941488266\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.11481576412916183\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.11485835909843445\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.1148986965417862\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.11493996530771255\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.11498347669839859\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.11502937227487564\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.11507681012153625\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.1151241883635521\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.11516937613487244\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.11521195620298386\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.1152525320649147\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.11529147624969482\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.1153298020362854\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.1153688058257103\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.11540916562080383\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.11544987559318542\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.11548993736505508\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.11552898585796356\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.11556731909513474\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.1156039834022522\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.11563961952924728\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.11567509174346924\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.11571058630943298\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.11574608832597733\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nloss pi: -0.11578112840652466\nadv: tensor([-0.6053, -1.0263, -1.4786,  ...,  0.5518,  0.3315,  0.0298])\nepisode: 1399 \t episode_reward: 14.125 \t total steps:29254\nepisode: 1499 \t episode_reward: 16.375 \t total steps:30932\nloss pi: -0.10584873706102371\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.10646317899227142\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.10747706890106201\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.10871454328298569\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.10996212810277939\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11096988618373871\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11144758015871048\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11124281585216522\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.1105496883392334\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.10999312996864319\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.10987032204866409\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11017120629549026\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11080086976289749\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11143913865089417\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11180541664361954\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11190562695264816\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11182215064764023\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.1116769015789032\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11155295372009277\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11149555444717407\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11152493208646774\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11163118481636047\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.1117902547121048\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11196437478065491\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11212235689163208\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11224105209112167\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11229881644248962\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.1123027503490448\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11227408051490784\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11224135756492615\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.1122354120016098\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11226694285869598\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11233245581388474\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11241482943296432\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11249182373285294\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11255130171775818\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11258725076913834\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11260680854320526\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11261703819036484\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11262519657611847\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11263864487409592\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.1126600056886673\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11269046366214752\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11272675544023514\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.1127656102180481\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11280178278684616\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11283287405967712\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11285781860351562\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11287683248519897\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11289267987012863\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11290678381919861\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11292295902967453\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11294248700141907\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11296530067920685\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.1129903718829155\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.1130143478512764\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11303690820932388\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11305801570415497\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11307612806558609\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11309286952018738\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11310866475105286\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11312444508075714\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11314117908477783\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11315886676311493\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11317742615938187\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11319644004106522\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11321516335010529\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11323338001966476\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11325094848871231\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11326704174280167\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11328272521495819\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.1132984608411789\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11331441253423691\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11333061009645462\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11334704607725143\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11336368322372437\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.1133805364370346\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11339743435382843\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11341389268636703\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nloss pi: -0.11342980712652206\nadv: tensor([-0.2091, -0.5005, -0.8559,  ...,  0.3429,  0.2168,  0.0244])\nepisode: 1599 \t episode_reward: 14.125 \t total steps:32390\nepisode: 1699 \t episode_reward: 13.375 \t total steps:33809\nepisode: 1799 \t episode_reward: 15.125 \t total steps:35302\nloss pi: -0.10514198243618011\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.10561376065015793\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.10645543038845062\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.1075228601694107\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.10859927535057068\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.10944849252700806\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.10982750356197357\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.1096041277050972\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.10901845246553421\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.10854479670524597\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.10855697840452194\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.10900600999593735\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.10957840085029602\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11001084744930267\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.1101936399936676\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11016634851694107\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11003663390874863\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.10990919917821884\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.10983363538980484\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.10983455181121826\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.10990653932094574\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.1100323349237442\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.1101851686835289\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11033409833908081\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11045227944850922\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11051183938980103\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.1105167418718338\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11049232631921768\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11045843362808228\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11044424772262573\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11046876013278961\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11052662879228592\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11059685051441193\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11065948754549026\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11071061342954636\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11073965579271317\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11075415462255478\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11075948178768158\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11076431721448898\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11077652126550674\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11080051958560944\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11083462834358215\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11087264865636826\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11090884357690811\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11093974113464355\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11096498370170593\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11098480969667435\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.1110014021396637\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11101670563220978\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11103448271751404\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11105583608150482\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11108069866895676\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11110678315162659\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11113185435533524\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11115565150976181\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11117701977491379\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11119568347930908\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11121337860822678\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11123143136501312\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11125041544437408\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11127113550901413\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11129216849803925\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11131297051906586\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.1113334521651268\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11135360598564148\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11137312650680542\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.1113915890455246\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.1114097312092781\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11142811924219131\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11144692450761795\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11146587878465652\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11148502677679062\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11150391399860382\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11152233928442001\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11154042184352875\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11155850440263748\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11157640069723129\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11159425228834152\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11161209642887115\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\nloss pi: -0.11162997782230377\nadv: tensor([0.8731, 0.7493, 0.6035,  ..., 0.6273, 0.2838, 0.3106])\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9aadc53a7dac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mppo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-211ea5e3cea6>\u001b[0m in \u001b[0;36mppo\u001b[0;34m(env, seed, buff_size, train_time_steps, gamma, clip_ratio, lr_pi, lr_vf, pi_train_itrs, v_train_itrs, lam, max_ep_len)\u001b[0m\n\u001b[1;32m     69\u001b[0m                         \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ret'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'obs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                         \u001b[0mvalue_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                        \u001b[0;31m# print(f'value_loss: {value_loss}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                         \u001b[0mvalue_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/domainrand/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2646\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ppo(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_shape(length, shape=None):\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = env.action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "*shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_shape(100 ,shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isscalar(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(100, *shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('domainrand': conda)",
   "language": "python",
   "name": "python37364bitdomainrandcondacc6e13f55da54f518b8a9788c42e3c45"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "122de4a4be327aa1ba4f735a2434ab4bffab9a9a54192dfc85777065cfde1c01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}