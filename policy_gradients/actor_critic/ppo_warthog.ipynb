{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akhil/anaconda3/envs/ppo/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/akhil/anaconda3/envs/ppo/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gym\n",
    "import torch\n",
    "import gym\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.distributions import Normal\n",
    "import scipy.signal\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "#torch.manual_seed(100)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_cumsum(x, discount):\n",
    "    \"\"\"\n",
    "    magic from rllab for computing discounted cumulative sums of vectors.\n",
    "    input: \n",
    "        vector x, \n",
    "        [x0, \n",
    "         x1, \n",
    "         x2]\n",
    "    output:\n",
    "        [x0 + discount * x1 + discount^2 * x2,  \n",
    "         x1 + discount * x2,\n",
    "         x2]\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, obs_dimension, sizes, act = nn.ReLU):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        sizes = [obs_dimension] + sizes + [1]\n",
    "        out_activation = nn.Identity\n",
    "        self.layers = []\n",
    "        for j in range(0,len(sizes) - 1):\n",
    "            act_l = act if j < len(sizes) -2 else out_activation\n",
    "            self.layers+=[nn.Linear(sizes[j], sizes[j+1]), act_l()]\n",
    "        self.v = nn.Sequential(*self.layers)\n",
    "    def forward(self, x):\n",
    "        return self.v(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetworkCat(nn.Module):\n",
    "    def __init__(self, obs_dimension, sizes, action_dimension, act= nn.ReLU):\n",
    "        super(PolicyNetworkCat, self).__init__()\n",
    "        sizes = [obs_dimension] + sizes + [action_dimension]\n",
    "        out_activation = nn.Identity\n",
    "        self.layers = []\n",
    "        for j in range(0,len(sizes) - 1):\n",
    "            act_l = act if j < len(sizes) -2 else out_activation\n",
    "            self.layers+=[nn.Linear(sizes[j], sizes[j+1]), act_l()]\n",
    "        self.pi = nn.Sequential(*self.layers)\n",
    "    def forward(self, x):\n",
    "        score = self.pi(x)\n",
    "        #probs = F.softmax(score,dim = 1)\n",
    "        dist = torch.distributions.Categorical(logits=score)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetworkGauss(nn.Module):\n",
    "    def __init__(self, obs_dimension, sizes, action_dimension):\n",
    "        super(PolicyNetworkGauss, self).__init__()\n",
    "        sizes = [obs_dimension] + sizes + [action_dimension]\n",
    "        act = nn.Tanh\n",
    "        out_activation = nn.Identity\n",
    "        self.layers = []\n",
    "        for j in range(0,len(sizes) - 1):\n",
    "            act_l = act if j < len(sizes) -2 else out_activation\n",
    "            self.layers+=[nn.Linear(sizes[j], sizes[j+1]), act_l()]\n",
    "        self.mu = nn.Sequential(*self.layers)\n",
    "        log_std = -0.5*np.ones(action_dimension, dtype=np.float32)\n",
    "        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std))\n",
    "    def forward(self, x):\n",
    "        mean = self.mu(x)\n",
    "        std = torch.exp(self.log_std)\n",
    "        dist = Normal(mean, std)\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "sizes = [128]\n",
    "obs_dimension = env.observation_space.shape\n",
    "action_dimension = env.action_space.shape\n",
    "print(obs_dimension)\n",
    "print(env.action_space.n)\n",
    "v = ValueNetwork(*obs_dimension, sizes)\n",
    "pi = PolicyNetworkGauss(*obs_dimension, sizes, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOBuffer:\n",
    "    \"\"\"\n",
    "    A buffer for storing trajectories experienced by a PPO agent interacting\n",
    "    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
    "    for calculating the advantages of state-action pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        #self.act_buf = np.zeros((size,), dtype=np.float32)\n",
    "        self.act_buf = np.zeros((size,act_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        \"\"\"\n",
    "        Append one timestep of agent-environment interaction to the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Call this at the end of a trajectory, or when one gets cut off\n",
    "        by an epoch ending. This looks back in the buffer to where the\n",
    "        trajectory started, and uses rewards and value estimates from\n",
    "        the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
    "        as well as compute the rewards-to-go for each state, to use as\n",
    "        the targets for the value function.\n",
    "\n",
    "        The \"last_val\" argument should be 0 if the trajectory ended\n",
    "        because the agent reached a terminal state (died), and otherwise\n",
    "        should be V(s_T), the value function estimated for the last state.\n",
    "        This allows us to bootstrap the reward-to-go calculation to account\n",
    "        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
    "        \"\"\"\n",
    "\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "       # print(rews)\n",
    "       # print(vals)\n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
    "        \n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n",
    "        \n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        Call this at the end of an epoch to get all of the data from\n",
    "        the buffer, with advantages appropriately normalized (shifted to have\n",
    "        mean zero and std one). Also, resets some pointers in the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # the next two lines implement the advantage normalization trick\n",
    "\n",
    "        self.adv_buf = (self.adv_buf - self.adv_buf.mean()) / self.adv_buf.std()\n",
    "        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf,\n",
    "                    adv=self.adv_buf, logp=self.logp_buf)\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in data.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo(env, seed = 0, buff_size = 4000, train_time_steps = 1000000, gamma = 0.99, clip_ratio = 0.2, lr_pi = 1e-3, \n",
    "        lr_vf = 1e-3, pi_train_itrs = 80, v_train_itrs = 80, lam = 0.97, max_ep_len = 500):\n",
    "        obs_dim = env.observation_space.shape\n",
    "        #action_dim = 2\n",
    "        action_dim = env.action_space.shape\n",
    "        h_sizes = [64,64]\n",
    "        vi = ValueNetwork(*obs_dim, h_sizes).to(device)\n",
    "        #pi = PolicyNetworkCat(*obs_dim, h_sizes, action_dim).to(device)\n",
    "        pi = PolicyNetworkGauss(*obs_dim, h_sizes, action_dim).to(device)\n",
    "        data_buff = PPOBuffer(*obs_dim, action_dim, buff_size)\n",
    "        policy_opt = optim.Adam(pi.parameters(), lr = lr_pi)\n",
    "        value_opt = optim.Adam(vi.parameters(), lr = lr_vf)\n",
    "        obs = env.reset()\n",
    "        curr_time_step = 0\n",
    "        pbar = tqdm(total = train_time_steps)\n",
    "        num_episode = 0\n",
    "        ep_rewards = [0]\n",
    "        while curr_time_step < train_time_steps: \n",
    "                for t in range(0, buff_size):\n",
    "                        with torch.no_grad():\n",
    "                                m = pi(torch.as_tensor(obs, dtype=torch.float32).to(device))\n",
    "                                action = m.sample()\n",
    "                                action = action.cpu().numpy() \n",
    "                                clipped_action = np.clip(env.action_space.low, env.action_space.high)\n",
    "                                logp = m.log_prob(a)\n",
    "                                #obs_new, rew, done, _ = env.step(a.item())\n",
    "                                obs_new, rew, done, _ = env.step(clipped_action)\n",
    "                                ep_rewards[num_episode]+=rew \n",
    "                                v = vi(torch.as_tensor(obs, dtype=torch.float32).to(device))\n",
    "                        data_buff.store(obs, action, rew, v.cpu().numpy(), logp.cpu().numpy())\n",
    "                        obs = obs_new\n",
    "                        if done or t == buff_size-1:\n",
    "                                if done:\n",
    "                                        v_ = 0.\n",
    "                                        obs = env.reset()\n",
    "                                        done = False\n",
    "                                        num_episode+=1\n",
    "                                        ep_rewards.append(0)\n",
    "                                        if num_episode %100 == 0:\n",
    "                                                print(f'episode: {num_episode-1} \\t episode_reward: {np.mean(ep_rewards[-10:-2])} \\t total steps:{curr_time_step}')\n",
    "                                else:\n",
    "                                        v_ = vi(torch.as_tensor(obs, dtype=torch.float32).to(device))\n",
    "                                        v_ = v_.detach().cpu().numpy()\n",
    "                                data_buff.finish_path(v_)\n",
    "                        curr_time_step+=1\n",
    "                        pbar.update(1)\n",
    "                data = data_buff.get()\n",
    "                ret, act, adv, o, logp_old= data['ret'].to(device), data['act'].to(device), data['adv'].to(device), data['obs'].to(device), data['logp'].to(device)\n",
    "                for j in range(0, pi_train_itrs):\n",
    "                        act_dist = pi(o)\n",
    "                        logp = act_dist.log_prob(act)\n",
    "                        ratio = torch.exp(logp - logp_old)\n",
    "                        clip_adv = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio) * adv\n",
    "                        loss_pi = -(torch.min(ratio * adv, clip_adv)).mean()\n",
    "                        loss_pi.backward()\n",
    "                        policy_opt.step()\n",
    "                for i in range (0, v_train_itrs):\n",
    "                        value_opt.zero_grad()\n",
    "                       # ret, ob = data['ret'], data['obs']\n",
    "                        val = vi(o)\n",
    "                        value_loss = F.mse_loss(val, ret)\n",
    "                        value_loss.backward()\n",
    "                        value_opt.step()\n",
    "                #pbar.update(1)\n",
    "        pbar.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2776/1000000 [00:00<04:07, 4021.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 99 \t episode_reward: 17.375 \t total steps:2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3994/1000000 [00:01<04:08, 4005.89it/s]/tmp/ipykernel_3640/789731038.py:56: UserWarning: Using a target size (torch.Size([4000])) that is different to the input size (torch.Size([4000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss = F.mse_loss(val, ret)\n",
      "  0%|          | 4805/1000000 [00:01<04:23, 3781.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 199 \t episode_reward: 31.625 \t total steps:4175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6900/1000000 [00:01<04:01, 4110.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 299 \t episode_reward: 16.875 \t total steps:6308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8967/1000000 [00:02<04:10, 3960.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 399 \t episode_reward: 17.75 \t total steps:8478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11463/1000000 [00:02<03:58, 4138.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 499 \t episode_reward: 21.625 \t total steps:10712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 13544/1000000 [00:03<04:03, 4042.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 599 \t episode_reward: 22.0 \t total steps:12857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 15600/1000000 [00:03<04:00, 4090.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 699 \t episode_reward: 22.625 \t total steps:15063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 17698/1000000 [00:04<03:58, 4113.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 799 \t episode_reward: 21.0 \t total steps:17224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 19770/1000000 [00:04<03:59, 4087.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 899 \t episode_reward: 25.5 \t total steps:19390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 22219/1000000 [00:05<04:02, 4036.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 999 \t episode_reward: 28.25 \t total steps:21761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 24672/1000000 [00:06<04:17, 3794.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1099 \t episode_reward: 20.125 \t total steps:24090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 27140/1000000 [00:06<03:58, 4076.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1199 \t episode_reward: 22.5 \t total steps:26361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 29198/1000000 [00:07<04:05, 3959.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1299 \t episode_reward: 25.625 \t total steps:28541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 31671/1000000 [00:07<03:56, 4088.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1399 \t episode_reward: 28.125 \t total steps:31198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 34171/1000000 [00:08<03:53, 4128.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1499 \t episode_reward: 20.5 \t total steps:33495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 36278/1000000 [00:09<04:13, 3799.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1599 \t episode_reward: 24.875 \t total steps:35930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 38735/1000000 [00:09<03:56, 4061.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1699 \t episode_reward: 27.625 \t total steps:38313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 41235/1000000 [00:10<03:59, 3999.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1799 \t episode_reward: 27.0 \t total steps:40532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 43290/1000000 [00:10<03:53, 4096.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1899 \t episode_reward: 15.0 \t total steps:42759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 45780/1000000 [00:11<03:54, 4064.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1999 \t episode_reward: 23.0 \t total steps:45337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 48305/1000000 [00:12<04:09, 3816.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2099 \t episode_reward: 27.625 \t total steps:47868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 51203/1000000 [00:12<03:50, 4113.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2199 \t episode_reward: 26.875 \t total steps:50440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 53675/1000000 [00:13<03:55, 4021.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2299 \t episode_reward: 29.625 \t total steps:53132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 56146/1000000 [00:14<04:05, 3838.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2399 \t episode_reward: 17.0 \t total steps:55462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 58152/1000000 [00:14<03:57, 3960.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2499 \t episode_reward: 24.0 \t total steps:57753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 61429/1000000 [00:15<03:53, 4014.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2599 \t episode_reward: 27.0 \t total steps:60648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 63939/1000000 [00:16<03:46, 4137.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2699 \t episode_reward: 27.0 \t total steps:63344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 66467/1000000 [00:16<03:50, 4050.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2799 \t episode_reward: 26.0 \t total steps:65930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 68962/1000000 [00:17<03:52, 4010.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2899 \t episode_reward: 32.25 \t total steps:68496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 71509/1000000 [00:17<03:40, 4217.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2999 \t episode_reward: 25.0 \t total steps:71071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 74046/1000000 [00:18<03:44, 4130.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3099 \t episode_reward: 22.5 \t total steps:73575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 76580/1000000 [00:19<03:50, 4006.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3199 \t episode_reward: 22.125 \t total steps:75893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 79103/1000000 [00:19<03:38, 4213.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3299 \t episode_reward: 23.75 \t total steps:78381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 81651/1000000 [00:20<03:41, 4142.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3399 \t episode_reward: 32.625 \t total steps:81083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 84173/1000000 [00:21<04:03, 3759.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3499 \t episode_reward: 29.5 \t total steps:83561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 86609/1000000 [00:21<03:47, 4019.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3599 \t episode_reward: 32.625 \t total steps:86127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 89485/1000000 [00:22<03:45, 4041.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3699 \t episode_reward: 32.75 \t total steps:88692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 92378/1000000 [00:23<03:57, 3820.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3799 \t episode_reward: 25.875 \t total steps:91674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 94859/1000000 [00:23<03:39, 4121.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3899 \t episode_reward: 29.125 \t total steps:94133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 97344/1000000 [00:24<03:44, 4016.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3999 \t episode_reward: 24.875 \t total steps:96855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 100274/1000000 [00:25<03:57, 3789.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4099 \t episode_reward: 34.875 \t total steps:99642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 102794/1000000 [00:25<03:36, 4140.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4199 \t episode_reward: 30.125 \t total steps:102352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 105678/1000000 [00:26<03:48, 3907.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4299 \t episode_reward: 33.0 \t total steps:105154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 108624/1000000 [00:27<03:43, 3987.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4399 \t episode_reward: 26.25 \t total steps:108036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 111528/1000000 [00:27<03:33, 4156.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4499 \t episode_reward: 33.875 \t total steps:111102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 114472/1000000 [00:28<03:34, 4123.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4599 \t episode_reward: 33.375 \t total steps:113756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 117420/1000000 [00:29<03:34, 4113.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4699 \t episode_reward: 28.25 \t total steps:116686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 119951/1000000 [00:29<03:28, 4221.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4799 \t episode_reward: 28.5 \t total steps:119638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 123666/1000000 [00:30<03:33, 4095.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4899 \t episode_reward: 30.125 \t total steps:122982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 126622/1000000 [00:31<03:28, 4189.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4999 \t episode_reward: 26.25 \t total steps:126195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 129958/1000000 [00:32<03:32, 4091.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5099 \t episode_reward: 22.375 \t total steps:129339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 132886/1000000 [00:33<03:37, 3980.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5199 \t episode_reward: 24.875 \t total steps:132320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 135811/1000000 [00:33<03:27, 4169.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5299 \t episode_reward: 27.375 \t total steps:135435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 139601/1000000 [00:34<03:26, 4159.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5399 \t episode_reward: 31.125 \t total steps:138918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 142941/1000000 [00:35<03:28, 4106.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5499 \t episode_reward: 35.125 \t total steps:142233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 145872/1000000 [00:36<03:27, 4122.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5599 \t episode_reward: 22.875 \t total steps:145349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 149239/1000000 [00:37<03:32, 4007.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5699 \t episode_reward: 36.5 \t total steps:148635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 152560/1000000 [00:37<03:34, 3946.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5799 \t episode_reward: 36.875 \t total steps:152033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 155912/1000000 [00:38<03:22, 4159.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5899 \t episode_reward: 27.875 \t total steps:155273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 159674/1000000 [00:39<03:22, 4149.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 5999 \t episode_reward: 23.0 \t total steps:158867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 163008/1000000 [00:40<03:21, 4148.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6099 \t episode_reward: 26.875 \t total steps:162254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 166363/1000000 [00:41<03:24, 4082.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6199 \t episode_reward: 31.25 \t total steps:165918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 170164/1000000 [00:42<03:18, 4186.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6299 \t episode_reward: 28.75 \t total steps:169661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 173899/1000000 [00:43<03:29, 3937.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6399 \t episode_reward: 40.0 \t total steps:173346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 177616/1000000 [00:44<03:21, 4073.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6499 \t episode_reward: 28.875 \t total steps:177041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 181855/1000000 [00:45<03:15, 4180.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6599 \t episode_reward: 33.375 \t total steps:181039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 185183/1000000 [00:46<03:24, 3978.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6699 \t episode_reward: 21.375 \t total steps:184453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 188887/1000000 [00:46<03:23, 3979.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6799 \t episode_reward: 37.875 \t total steps:188447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 192691/1000000 [00:47<03:23, 3961.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6899 \t episode_reward: 39.125 \t total steps:192262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 196865/1000000 [00:48<03:18, 4040.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6999 \t episode_reward: 46.875 \t total steps:196161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 200659/1000000 [00:49<03:19, 4004.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7099 \t episode_reward: 50.5 \t total steps:200218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 204845/1000000 [00:50<03:17, 4026.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7199 \t episode_reward: 37.125 \t total steps:204118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 208682/1000000 [00:51<03:18, 3986.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7299 \t episode_reward: 51.0 \t total steps:208060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 212119/1000000 [00:52<03:18, 3968.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7399 \t episode_reward: 43.75 \t total steps:211780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 216350/1000000 [00:53<03:27, 3777.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7499 \t episode_reward: 41.125 \t total steps:215696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 220556/1000000 [00:54<03:18, 3918.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7599 \t episode_reward: 43.0 \t total steps:219913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 224744/1000000 [00:55<03:17, 3929.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7699 \t episode_reward: 45.875 \t total steps:224166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 228891/1000000 [00:56<03:15, 3938.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7799 \t episode_reward: 32.5 \t total steps:228349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 233384/1000000 [00:57<03:15, 3930.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7899 \t episode_reward: 32.75 \t total steps:232728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 237517/1000000 [00:58<03:10, 3993.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7999 \t episode_reward: 41.25 \t total steps:237065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 242072/1000000 [01:00<03:05, 4075.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8099 \t episode_reward: 44.75 \t total steps:241280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 246662/1000000 [01:01<03:04, 4087.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8199 \t episode_reward: 55.5 \t total steps:245942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 251254/1000000 [01:02<02:59, 4171.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8299 \t episode_reward: 38.625 \t total steps:250434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 255885/1000000 [01:03<02:58, 4166.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8399 \t episode_reward: 35.125 \t total steps:255133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 260074/1000000 [01:04<03:11, 3859.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8499 \t episode_reward: 43.875 \t total steps:259715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 265111/1000000 [01:05<03:01, 4037.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8599 \t episode_reward: 59.0 \t total steps:264286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 269685/1000000 [01:06<03:00, 4043.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8699 \t episode_reward: 51.625 \t total steps:269007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 274276/1000000 [01:07<02:55, 4126.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8799 \t episode_reward: 40.0 \t total steps:273662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 279285/1000000 [01:09<02:53, 4159.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8899 \t episode_reward: 65.25 \t total steps:278647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 283886/1000000 [01:10<02:51, 4176.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8999 \t episode_reward: 41.125 \t total steps:283248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 288995/1000000 [01:11<02:55, 4061.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 9099 \t episode_reward: 47.25 \t total steps:288191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 293603/1000000 [01:12<02:53, 4078.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 9199 \t episode_reward: 34.25 \t total steps:292848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 298631/1000000 [01:13<02:47, 4191.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 9299 \t episode_reward: 53.25 \t total steps:298121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 304116/1000000 [01:15<02:58, 3895.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 9399 \t episode_reward: 53.0 \t total steps:303429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 309569/1000000 [01:16<02:48, 4106.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 9499 \t episode_reward: 46.125 \t total steps:308823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 315062/1000000 [01:17<02:42, 4219.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 9599 \t episode_reward: 52.0 \t total steps:314496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 320135/1000000 [01:19<02:55, 3874.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 9699 \t episode_reward: 61.875 \t total steps:319409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 325536/1000000 [01:20<02:47, 4032.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 9799 \t episode_reward: 57.875 \t total steps:324869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 330575/1000000 [01:21<02:40, 4160.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 9899 \t episode_reward: 47.875 \t total steps:330127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 336452/1000000 [01:23<02:47, 3956.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 9999 \t episode_reward: 61.5 \t total steps:335907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 342336/1000000 [01:24<02:38, 4158.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10099 \t episode_reward: 66.0 \t total steps:341698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 347769/1000000 [01:26<02:35, 4189.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10199 \t episode_reward: 55.875 \t total steps:347033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 353657/1000000 [01:27<02:39, 4063.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10299 \t episode_reward: 63.0 \t total steps:352925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 358717/1000000 [01:28<02:32, 4205.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10399 \t episode_reward: 41.375 \t total steps:358217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 364687/1000000 [01:30<02:40, 3970.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10499 \t episode_reward: 64.375 \t total steps:364005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 370559/1000000 [01:31<02:31, 4156.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10599 \t episode_reward: 60.25 \t total steps:370098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 376880/1000000 [01:33<02:33, 4049.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10699 \t episode_reward: 61.5 \t total steps:376059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 382742/1000000 [01:34<02:27, 4193.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10799 \t episode_reward: 60.0 \t total steps:381892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 389502/1000000 [01:36<02:28, 4106.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10899 \t episode_reward: 74.875 \t total steps:388987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 395795/1000000 [01:37<02:25, 4155.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10999 \t episode_reward: 68.125 \t total steps:395268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 402091/1000000 [01:39<02:25, 4098.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 11099 \t episode_reward: 73.125 \t total steps:401604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 408340/1000000 [01:40<02:35, 3811.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 11199 \t episode_reward: 69.0 \t total steps:407927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 415028/1000000 [01:42<02:20, 4177.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 11299 \t episode_reward: 58.375 \t total steps:414441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 421312/1000000 [01:44<02:22, 4053.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 11399 \t episode_reward: 76.0 \t total steps:420845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 427973/1000000 [01:45<02:17, 4148.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 11499 \t episode_reward: 63.0 \t total steps:427460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 435070/1000000 [01:47<02:15, 4169.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 11599 \t episode_reward: 61.625 \t total steps:434416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 441765/1000000 [01:49<02:15, 4121.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 11699 \t episode_reward: 85.5 \t total steps:440983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 448524/1000000 [01:50<02:18, 3992.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 11799 \t episode_reward: 67.0 \t total steps:448085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 455717/1000000 [01:52<02:09, 4188.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 11899 \t episode_reward: 76.75 \t total steps:455148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 462914/1000000 [01:54<02:07, 4220.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 11999 \t episode_reward: 64.125 \t total steps:462241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 470080/1000000 [01:56<02:08, 4113.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 12099 \t episode_reward: 87.125 \t total steps:469548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 477320/1000000 [01:57<02:06, 4139.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 12199 \t episode_reward: 59.5 \t total steps:476877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 484530/1000000 [01:59<02:07, 4039.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 12299 \t episode_reward: 70.0 \t total steps:484001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 492220/1000000 [02:01<02:07, 3996.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 12399 \t episode_reward: 55.875 \t total steps:491704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 501042/1000000 [02:03<02:03, 4055.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 12499 \t episode_reward: 84.0 \t total steps:500580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 509867/1000000 [02:05<02:01, 4034.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 12599 \t episode_reward: 92.125 \t total steps:509425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 519451/1000000 [02:08<01:53, 4221.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 12699 \t episode_reward: 108.0 \t total steps:518738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 529257/1000000 [02:10<01:56, 4056.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 12799 \t episode_reward: 95.625 \t total steps:528587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 538919/1000000 [02:12<01:49, 4193.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 12899 \t episode_reward: 95.0 \t total steps:538489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 550225/1000000 [02:15<01:47, 4176.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 12999 \t episode_reward: 106.625 \t total steps:549546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 562144/1000000 [02:18<01:48, 4042.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 13099 \t episode_reward: 118.875 \t total steps:561599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 573733/1000000 [02:21<01:45, 4056.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 13199 \t episode_reward: 123.125 \t total steps:572904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 585906/1000000 [02:24<01:39, 4144.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 13299 \t episode_reward: 124.125 \t total steps:585217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 593980/1000000 [02:26<01:37, 4158.94it/s]"
     ]
    }
   ],
   "source": [
    "#python.dataScience.textOutputLimit = 0\n",
    "ppo(env)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ppo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "122de4a4be327aa1ba4f735a2434ab4bffab9a9a54192dfc85777065cfde1c01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
